{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "> In the K-Means algorithm, `k` represents the number of clusters you have in your dataset. -- Josh Bernhard\n",
    "\n",
    "One key step in this algorithm is determining the number of cluster (`k`) in our dataset. Sometimes we can visualize the data and it's easy to tell. Other times, we have prior knowledge and it's also an easy choice. But there are times when we have no idea and, for these cases, we can use the Elbow Method.\n",
    "\n",
    "## Elbow Method\n",
    "\n",
    "<br><img src=\"images/elbow-method-1.png\" width=\"720px\"><br>\n",
    "\n",
    "This method uses different numbers of clusters, then calculate the average distance of the points to the center of the cluster. For smaller values of `k`, there's a big decrease in this calculation, and after some value this decrease gets small.\n",
    "\n",
    "<br><img src=\"images/elbow-method-2.png\" width=\"720px\"><br>\n",
    "\n",
    "The ideal number of clusters is the one just before the decrease in average distance gets small, in this example, 4.\n",
    "\n",
    "<br><img src=\"images/elbow-method-3.png\" width=\"720px\"><br>\n",
    "\n",
    "PS: The plot above is called **Scree Plot**.\n",
    "\n",
    "PS2: The optimal point above is called **Elbow**.\n",
    "\n",
    "## How does K-Means work?\n",
    "\n",
    "<br><img src=\"images/k-means-step-1.png\" width=\"720px\"><br>\n",
    "\n",
    "The process for K-Means is iterative.\n",
    "\n",
    "We start by randomly adding the centroid to the data points.\n",
    "\n",
    "<br><img src=\"images/k-means-step-2.png\" width=\"720px\"><br>\n",
    "\n",
    "Then we associate each point with the closest centroid.\n",
    "\n",
    "<br><img src=\"images/k-means-step-3.png\" width=\"720px\"><br>\n",
    "\n",
    "Next up, we calculate the distances and place the centroid on the center of the points.\n",
    "\n",
    "<br><img src=\"images/k-means-step-4.png\" width=\"720px\"><br>\n",
    "\n",
    "After this step, points may be closer to a different centroid than the initially associated. We repeat these steps until the centroids no longer move.\n",
    "\n",
    "<br><img src=\"images/k-means-step-5.png\" width=\"720px\">\n",
    "\n",
    "### Optimal solution\n",
    "\n",
    "There are two areas of concern regarding the way K-Means work:\n",
    "\n",
    "1. The random starting points of the algorithm matter.\n",
    "2. The final clusters may be different depending on the starting points.\n",
    "\n",
    "That said, to protect against local minima, it's important to use repeated runs with different starting points. The best solution will be the one(s) with smallest average distance!\n",
    "\n",
    "PS: `scikit-learn` does this automatically for us!\n",
    "\n",
    "## Feature scaling\n",
    "\n",
    "Just like in supervised learning, feature scaling plays an important role in unsupervised learning algorithms that uses distances as part of its optimization. We'll be using stardardizing - or z-score scaling, which is usually used in clustering -, and normalizing - or min-max scaling, when dealing with coloring images.\n",
    "\n",
    "If we do not scale our features, those with larger variance will dominate the importance in clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
